{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6b9c47fd-5631-4f71-999c-39141186ac33",
   "metadata": {},
   "source": [
    "# ray_baseline.ipynb\n",
    "\n",
    "Baseline model serving implementation from the [benchmark notebook](./benchmark.ipynb).\n",
    "\n",
    "This is the serving code we used before porting to TorchServe (see [this notebok](./torchserve.ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c8c73a81-6f3d-40a4-a0c7-20a955d4aab4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialization and import code goes in this cell.\n",
    "\n",
    "# Imports: Python core, then third-party, then local.\n",
    "# Try to keep each block in alphabetical order, or the linter may get angry.\n",
    "\n",
    "import requests\n",
    "import starlette\n",
    "import time\n",
    "import os\n",
    "import json\n",
    "\n",
    "import scipy.special\n",
    "\n",
    "import ray\n",
    "from ray import serve\n",
    "import torch\n",
    "import transformers\n",
    "\n",
    "# Fix silly warning messages about parallel tokenizers\n",
    "os.environ['TOKENIZERS_PARALLELISM'] = 'False'\n",
    "\n",
    "\n",
    "# Reduce the volume of warning messages from `transformers`\n",
    "transformers.logging.set_verbosity_error()\n",
    "\n",
    "\n",
    "def reboot_ray():\n",
    "    if ray.is_initialized():\n",
    "        ray.shutdown()\n",
    "\n",
    "    if torch.cuda.is_available():\n",
    "        return ray.init(num_gpus=1)\n",
    "    else:\n",
    "        return ray.init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "cfec84ca-c245-44ec-b5e3-4d68aa6ea093",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Constants go here\n",
    "INTENT_MODEL_NAME = 'mrm8488/t5-base-finetuned-e2m-intent'\n",
    "SENTIMENT_MODEL_NAME = 'cardiffnlp/twitter-roberta-base-sentiment'\n",
    "QA_MODEL_NAME = 'deepset/roberta-base-squad2'\n",
    "GENERATE_MODEL_NAME = 'gpt2'\n",
    "\n",
    "\n",
    "INTENT_INPUT = {\n",
    "    'context':\n",
    "        (\"I came here to eat chips and beat you up, \"\n",
    "         \"and I'm all out of chips.\")\n",
    "}\n",
    "\n",
    "SENTIMENT_INPUT = {\n",
    "    'context': \"We're not happy unless you're not happy.\"\n",
    "}\n",
    "\n",
    "QA_INPUT = {\n",
    "    'question': 'What is 1 + 1?',\n",
    "    'context': \n",
    "        \"\"\"Addition (usually signified by the plus symbol +) is one of the four basic operations of \n",
    "        arithmetic, the other three being subtraction, multiplication and division. The addition of two \n",
    "        whole numbers results in the total amount or sum of those values combined. The example in the\n",
    "        adjacent image shows a combination of three apples and two apples, making a total of five apples. \n",
    "        This observation is equivalent to the mathematical expression \"3 + 2 = 5\" (that is, \"3 plus 2 \n",
    "        is equal to 5\").\n",
    "        \"\"\"\n",
    "}\n",
    "\n",
    "GENERATE_INPUT = {\n",
    "    'prompt_text': 'All your base are'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3111006c-4a4a-4301-9e15-cc761ddb33ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-02-25 17:22:22,792\tINFO services.py:1374 -- View the Ray dashboard at \u001b[1m\u001b[32mhttp://127.0.0.1:8265\u001b[39m\u001b[22m\n",
      "\u001b[2m\u001b[36m(ServeController pid=80034)\u001b[0m 2022-02-25 17:22:27,325\tINFO checkpoint_path.py:16 -- Using RayInternalKVStore for controller checkpoint and recovery.\n",
      "\u001b[2m\u001b[36m(ServeController pid=80034)\u001b[0m 2022-02-25 17:22:27,437\tINFO http_state.py:98 -- Starting HTTP proxy with name 'SERVE_CONTROLLER_ACTOR:vxzeES:SERVE_PROXY_ACTOR-node:127.0.0.1-0' on node 'node:127.0.0.1-0' listening on '127.0.0.1:8000'\n",
      "2022-02-25 17:22:27,924\tINFO api.py:475 -- Started Serve instance in namespace '0ba90253-709e-4241-bae1-436aa41f0d8c'.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<ray.serve.api.Client at 0x7ff46849b7c0>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "serve.shutdown()\n",
    "reboot_ray()\n",
    "serve.start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c7bd5491-7be0-496a-b612-c2cfa91978dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Intent:\n",
    "    def __init__(self):\n",
    "        # Tokenizer loading code from the model zoo doesn't work, so we \n",
    "        # explicitly specify the t5-base tokenizer.\n",
    "        self._tokenizer = transformers.AutoTokenizer.from_pretrained('t5-base')\n",
    "        self._model = transformers.AutoModelForSeq2SeqLM.from_pretrained(\n",
    "            INTENT_MODEL_NAME)\n",
    "        self._max_length = 128  # Max sequence length, input + output, in tokens\n",
    "\n",
    "    async def __call__(self, request: starlette.requests.Request):\n",
    "        json_request = await request.json()\n",
    "\n",
    "        # Preprocessing\n",
    "        input_text = f'{json_request[\"context\"]} </s>'\n",
    "        features = self._tokenizer([input_text], return_tensors='pt')\n",
    "\n",
    "        # Inference\n",
    "        output = self._model.generate(\n",
    "            input_ids=features['input_ids'], \n",
    "            attention_mask=features['attention_mask'],\n",
    "            max_length=self._max_length)\n",
    "\n",
    "        # Postprocessing\n",
    "        result_string = self._tokenizer.decode(output[0])\n",
    "        result_string = result_string.replace('<pad>', '')\n",
    "        result_string = result_string[len(' '):-len('</s>')]\n",
    "\n",
    "        return {\n",
    "            \"intent\": result_string\n",
    "        }\n",
    "\n",
    "\n",
    "class Sentiment:\n",
    "    def __init__(self):\n",
    "        self._tokenizer = transformers.AutoTokenizer.from_pretrained(\n",
    "            SENTIMENT_MODEL_NAME)\n",
    "        self._model = (transformers.AutoModelForSequenceClassification\n",
    "                       .from_pretrained(SENTIMENT_MODEL_NAME))\n",
    "\n",
    "    async def __call__(self, request: starlette.requests.Request):\n",
    "        json_request = await request.json()\n",
    "\n",
    "        # Preprocessing\n",
    "        encoded_input = self._tokenizer(json_request['context'],\n",
    "                                        return_tensors='pt')\n",
    "\n",
    "        # Inference\n",
    "        output = self._model(**encoded_input)\n",
    "\n",
    "        # Postprocessing\n",
    "        scores = output[0][0].detach().numpy()\n",
    "        scores = scipy.special.softmax(scores)\n",
    "        scores = [float(s) for s in scores]\n",
    "        scores = {k: v for k, v in zip(['positive', 'neutral', 'negative'],\n",
    "                                       scores)}\n",
    "        return scores\n",
    "\n",
    "\n",
    "class QA:\n",
    "    def __init__(self):\n",
    "        self._pipeline = transformers.pipeline(\n",
    "            'question-answering', model=QA_MODEL_NAME)\n",
    "\n",
    "    async def __call__(self, request: starlette.requests.Request):\n",
    "        json_request = await request.json()\n",
    "\n",
    "        # Preprocessing (returns a Python generator)\n",
    "        qa_pre = self._pipeline.create_sample(**json_request)\n",
    "        qa_pre = self._pipeline.preprocess(qa_pre)\n",
    "\n",
    "        # Inference\n",
    "        qa_output = (self._pipeline.forward(example) for example in qa_pre)\n",
    "\n",
    "        # Postprocessing\n",
    "        qa_result = self._pipeline.postprocess(qa_output)\n",
    "\n",
    "        return qa_result\n",
    "\n",
    "\n",
    "class Generate:\n",
    "    def __init__(self):\n",
    "        self._pipeline = transformers.pipeline(\n",
    "            'text-generation', model=GENERATE_MODEL_NAME)\n",
    "        self._pad_token_id = self._pipeline.tokenizer.eos_token_id\n",
    "\n",
    "    async def __call__(self, request: starlette.requests.Request):\n",
    "        json_request = await request.json()\n",
    "\n",
    "        # Preprocessing\n",
    "        generate_pre = self._pipeline.preprocess(**json_request)\n",
    "\n",
    "        # Inference\n",
    "        generate_output = self._pipeline.forward(\n",
    "            generate_pre, pad_token_id=self._pad_token_id)\n",
    "\n",
    "        # Postprocessing\n",
    "        generate_result = self._pipeline.postprocess(generate_output)\n",
    "\n",
    "        return generate_result[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60fa3569-8f71-4158-a1ce-a47d1c0da78a",
   "metadata": {},
   "source": [
    "Now we can deploy all of these pipelines as Serve endpoints."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "02814570-6aeb-4cc3-be48-a0efbc878b50",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-02-25 17:22:27,963\tINFO api.py:249 -- Updating deployment 'intent_en'. component=serve deployment=intent_en\n",
      "2022-02-25 17:22:27,971\tINFO api.py:249 -- Updating deployment 'sentiment_en'. component=serve deployment=sentiment_en\n",
      "2022-02-25 17:22:27,980\tINFO api.py:249 -- Updating deployment 'qa_en'. component=serve deployment=qa_en\n",
      "2022-02-25 17:22:27,989\tINFO api.py:249 -- Updating deployment 'generate_en'. component=serve deployment=generate_en\n",
      "2022-02-25 17:22:27,999\tINFO api.py:249 -- Updating deployment 'intent_es'. component=serve deployment=intent_es\n",
      "2022-02-25 17:22:28,010\tINFO api.py:249 -- Updating deployment 'sentiment_es'. component=serve deployment=sentiment_es\n",
      "\u001b[2m\u001b[36m(HTTPProxyActor pid=80044)\u001b[0m INFO:     Started server process [80044]\n",
      "2022-02-25 17:22:28,022\tINFO api.py:249 -- Updating deployment 'qa_es'. component=serve deployment=qa_es\n",
      "\u001b[2m\u001b[36m(ServeController pid=80034)\u001b[0m 2022-02-25 17:22:28,025\tINFO deployment_state.py:920 -- Adding 1 replicas to deployment 'intent_en'. component=serve deployment=intent_en\n",
      "\u001b[2m\u001b[36m(ServeController pid=80034)\u001b[0m 2022-02-25 17:22:28,044\tINFO deployment_state.py:920 -- Adding 1 replicas to deployment 'sentiment_en'. component=serve deployment=sentiment_en\n",
      "\u001b[2m\u001b[36m(ServeController pid=80034)\u001b[0m 2022-02-25 17:22:28,061\tINFO deployment_state.py:920 -- Adding 1 replicas to deployment 'qa_en'. component=serve deployment=qa_en\n",
      "\u001b[2m\u001b[36m(ServeController pid=80034)\u001b[0m 2022-02-25 17:22:28,081\tINFO deployment_state.py:920 -- Adding 1 replicas to deployment 'generate_en'. component=serve deployment=generate_en\n",
      "\u001b[2m\u001b[36m(ServeController pid=80034)\u001b[0m 2022-02-25 17:22:28,100\tINFO deployment_state.py:920 -- Adding 1 replicas to deployment 'intent_es'. component=serve deployment=intent_es\n",
      "2022-02-25 17:22:28,185\tINFO api.py:249 -- Updating deployment 'generate_es'. component=serve deployment=generate_es\n",
      "2022-02-25 17:22:28,202\tINFO api.py:249 -- Updating deployment 'intent_zh'. component=serve deployment=intent_zh\n",
      "2022-02-25 17:22:28,222\tINFO api.py:249 -- Updating deployment 'sentiment_zh'. component=serve deployment=sentiment_zh\n",
      "\u001b[2m\u001b[36m(ServeController pid=80034)\u001b[0m 2022-02-25 17:22:28,120\tINFO deployment_state.py:920 -- Adding 1 replicas to deployment 'sentiment_es'. component=serve deployment=sentiment_es\n",
      "\u001b[2m\u001b[36m(ServeController pid=80034)\u001b[0m 2022-02-25 17:22:28,146\tINFO deployment_state.py:920 -- Adding 1 replicas to deployment 'qa_es'. component=serve deployment=qa_es\n",
      "2022-02-25 17:22:28,243\tINFO api.py:249 -- Updating deployment 'qa_zh'. component=serve deployment=qa_zh\n",
      "2022-02-25 17:22:28,263\tINFO api.py:249 -- Updating deployment 'generate_zh'. component=serve deployment=generate_zh\n",
      "\u001b[2m\u001b[36m(ServeController pid=80034)\u001b[0m 2022-02-25 17:22:28,275\tINFO deployment_state.py:920 -- Adding 1 replicas to deployment 'generate_es'. component=serve deployment=generate_es\n",
      "\u001b[2m\u001b[36m(ServeController pid=80034)\u001b[0m 2022-02-25 17:22:28,295\tINFO deployment_state.py:920 -- Adding 1 replicas to deployment 'intent_zh'. component=serve deployment=intent_zh\n",
      "\u001b[2m\u001b[36m(ServeController pid=80034)\u001b[0m 2022-02-25 17:22:28,324\tINFO deployment_state.py:920 -- Adding 1 replicas to deployment 'sentiment_zh'. component=serve deployment=sentiment_zh\n",
      "\u001b[2m\u001b[36m(ServeController pid=80034)\u001b[0m 2022-02-25 17:22:28,355\tINFO deployment_state.py:920 -- Adding 1 replicas to deployment 'qa_zh'. component=serve deployment=qa_zh\n",
      "\u001b[2m\u001b[36m(ServeController pid=80034)\u001b[0m 2022-02-25 17:22:28,385\tINFO deployment_state.py:920 -- Adding 1 replicas to deployment 'generate_zh'. component=serve deployment=generate_zh\n",
      "\u001b[2m\u001b[36m(intent_en pid=80042)\u001b[0m The `xla_device` argument has been deprecated in v4.4.0 of Transformers. It is ignored and you can safely remove it from your `config.json` file.\n",
      "\u001b[2m\u001b[36m(intent_es pid=80040)\u001b[0m The `xla_device` argument has been deprecated in v4.4.0 of Transformers. It is ignored and you can safely remove it from your `config.json` file.\n",
      "\u001b[2m\u001b[36m(intent_zh pid=80032)\u001b[0m The `xla_device` argument has been deprecated in v4.4.0 of Transformers. It is ignored and you can safely remove it from your `config.json` file.\n"
     ]
    }
   ],
   "source": [
    "# Define endpoints\n",
    "LANGUAGES = ['en', 'es', 'zh']\n",
    "MAX_CONCURRENT_QUERIES = 1\n",
    "\n",
    "deployments = {}\n",
    "for lang in LANGUAGES:\n",
    "    deployments[(lang, 'intent')] = (\n",
    "        serve.deployment(Intent, f'intent_{lang}', \n",
    "                         route_prefix=f'/predictions/intent_{lang}',\n",
    "                         max_concurrent_queries=MAX_CONCURRENT_QUERIES))\n",
    "    deployments[(lang, 'sentiment')] = (\n",
    "        serve.deployment(Sentiment, f'sentiment_{lang}',\n",
    "                         route_prefix=f'/predictions/sentiment_{lang}',\n",
    "                         max_concurrent_queries=MAX_CONCURRENT_QUERIES))\n",
    "    deployments[(lang, 'qa')] = (\n",
    "        serve.deployment(QA, f'qa_{lang}',\n",
    "                         route_prefix=f'/predictions/qa_{lang}',\n",
    "                         max_concurrent_queries=MAX_CONCURRENT_QUERIES))\n",
    "    deployments[(lang, 'generate')] = (\n",
    "        serve.deployment(Generate, f'generate_{lang}', \n",
    "                         route_prefix=f'/predictions/generate_{lang}',\n",
    "                         max_concurrent_queries=MAX_CONCURRENT_QUERIES))\n",
    "\n",
    "\n",
    "for d in deployments.values():\n",
    "    d.deploy(_blocking=False)\n",
    "\n",
    "# Wait a moment so log output doesn't go to the next cell's output\n",
    "time.sleep(10.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "baac194f-9c7e-4d7c-8228-4e55b31738f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Intent result: {'intent': 'to eat'}\n",
      "Sentiment result: {'positive': 0.5419477820396423, 'neutral': 0.38251084089279175, 'negative': 0.07554134726524353}\n",
      "Question answering result: {'score': 4.278938831703272e-06, 'start': 483, 'end': 484, 'answer': '5'}\n",
      "Natural language generation result: {'generated_text': 'All your base are in the red zone\\n\\n\\nI had a bit of trouble finding him, but I ended up putting a red card on him because of his green.\\n\\n\\nHe will probably be given a lot of respect for his blue, but'}\n"
     ]
    }
   ],
   "source": [
    "intent_result = requests.put(\n",
    "    'http://127.0.0.1:8000/predictions/intent_en', \n",
    "    json.dumps(INTENT_INPUT)).json()\n",
    "print(f'Intent result: {intent_result}')\n",
    "\n",
    "sentiment_result = requests.put(\n",
    "    'http://127.0.0.1:8000/predictions/sentiment_en', \n",
    "    json.dumps(SENTIMENT_INPUT)).json()\n",
    "print(f'Sentiment result: {sentiment_result}')\n",
    "\n",
    "qa_result = requests.put(\n",
    "    'http://127.0.0.1:8000/predictions/qa_en', \n",
    "    json.dumps(QA_INPUT)).json()\n",
    "print(f'Question answering result: {qa_result}')\n",
    "\n",
    "generate_result = requests.put(\n",
    "    'http://127.0.0.1:8000/predictions/generate_en', \n",
    "    json.dumps(GENERATE_INPUT)).json()\n",
    "print(f'Natural language generation result: {generate_result}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d12c5007-d2dc-4711-8ab0-fc36d067db34",
   "metadata": {},
   "outputs": [],
   "source": [
    "ray.shutdown()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42017025-37d7-4dd1-b436-ae4fea31bc94",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
